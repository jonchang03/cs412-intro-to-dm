{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "In this programming assignment, you are required to implement a contiguous sequential pattern mining algorithm and apply it on text data to mine potential phrase candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "The provided input file (\"reviews_sample.txt\") consists of 10,000 online reviews from Yelp users. The reviews have been stemmed (to remove the postfix of each word so words with similar semantics can have the same form), and most of the punctuation has been removed. Therefore, each line is basically a list of strings separated by spaces.\n",
    "\n",
    "An example line is provided below:\n",
    "\n",
    "`cold cheap beer good bar food good service looking great pittsburgh style fish \n",
    "sandwich place breading light fish plentiful good side home cut fry good \n",
    "grilled chicken salad steak soup day homemade lot special great place lunch \n",
    "bar snack beer`\n",
    "\n",
    "## Task\n",
    "You need to implement an algorithm to mine contiguous sequential patterns that are frequent in the input data. A contiguous sequential pattern is a sequence of items that frequently appears as a consecutive subsequence in a database of many sequences. For example, if the database is\n",
    "\n",
    "`A,B,A,C\n",
    "A,C,A,B,A,B\n",
    "B,A,A,C,D`\n",
    "\n",
    "and the minimum support is 2, then patterns like \"A,B,A\" or \"A,C\" are both frequent contiguous sequential patterns, while the pattern \"A,A\" is not a frequent contiguous sequential pattern because in the first two sequences the two A's are not consecutive to each other. Notice that it is still a frequent sequential pattern though.\n",
    "\n",
    "Also, notice that multiple appearances of a subsequence in a single sequence record only counts once. For example, the pattern \"A,B\" appears 1 time in the first sequence and 2 times in the second, but its support should be calculated as 2, as there are only 2 records containing subsequence \"A,B\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPADE Implementation\n",
    "https://github.com/shahin/sequenceminer/blob/master/sequenceminer/miner.py\n",
    "\n",
    "### Notes:\n",
    "* Python 3 dictionary uses dict.items() instead of dict.iteritems()\n",
    "    * https://stackoverflow.com/questions/30418481/error-dict-object-has-no-attribute-iteritems-when-trying-to-use-networkx\n",
    "* Python 3, dict.keys() returns iterable but not indexable object\n",
    "    * https://stackoverflow.com/questions/26394748/nltk-python-error-typeerror-dict-keys-object-is-not-subscriptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class _KeyDefaultDict(collections.defaultdict):\n",
    "    def __missing__(self,key):\n",
    "        if self.default_factory is None:\n",
    "            raise KeyError(key)\n",
    "        else:\n",
    "            ret = self[key] = self.default_factory(key)\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "# from keydefaultdict import _KeyDefaultDict \n",
    "\n",
    "Event = collections.namedtuple('Event','sid eid')\n",
    "\n",
    "class Element(object):\n",
    "    '''An element of the set of all possible subsequences, and a description of\n",
    "    where that element occurs in the input sequences.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,seq,*events):\n",
    "\n",
    "        self.seq = seq\n",
    "        self.events = set()\n",
    "\n",
    "        for event in events:\n",
    "            self.events.add(event)\n",
    "\n",
    "    def __ior__(self,other_element):\n",
    "        '''Implements the assignment operator |= by returning an Element whose\n",
    "        events attribute is a union of the events of both input Elements.\n",
    "        '''\n",
    "\n",
    "        self.events |= other_element.events\n",
    "        return self\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        return self.__dict__.__repr__()\n",
    "\n",
    "    def __eq__(self,other):\n",
    "\n",
    "        return (self.seq == other.seq and self.events == other.events)\n",
    "\n",
    "      \n",
    "def subset_to_support(elements,support_threshold):\n",
    "    '''Given an IdList, return an IdList containing only those atoms which\n",
    "    meet the support threshold.\n",
    "    '''\n",
    "\n",
    "    subsetted = _KeyDefaultDict(Element)\n",
    "\n",
    "    for element_name,element in elements.items():\n",
    "        support = len(set([event.sid for event in element.events]))\n",
    "        if support >= support_threshold:\n",
    "            subsetted[element_name] = element\n",
    "                    \n",
    "    return subsetted\n",
    "\n",
    "\n",
    "def count_frequent_two_seq(elements,support_threshold):\n",
    "    '''Given an IdList of atoms, return a dictionary of two-sequences as keys with\n",
    "    the frequency of each two-sequence as the value.\n",
    "    '''\n",
    "\n",
    "    # Given an dictionary of Elements, convert it to a horizontal ID list in order to\n",
    "    # count the frequency of each two-sequence of atoms.\n",
    "    horizontal_db = {} \n",
    "\n",
    "    for element_name,element in elements.items():\n",
    "                \n",
    "        for event in element.events:\n",
    "\n",
    "            if event.sid not in horizontal_db:\n",
    "                 horizontal_db[event.sid] = []\n",
    "\n",
    "            horizontal_db[event.sid].append((element_name,event.eid))\n",
    "\n",
    "    # create counts using horizontal_db\n",
    "    counts = collections.defaultdict(int)\n",
    "    \n",
    "    # for sid,seq in horizontal_db.iteritems():\n",
    "    for sid,seq in horizontal_db.items():\n",
    "        \n",
    "        for event_index_i,event_i in enumerate(seq):\n",
    "            for event_index_j,event_j in enumerate(seq[event_index_i+1:]):\n",
    "                        \n",
    "                if event_i[1] <= event_j[1]:\n",
    "                    two_seq = event_i[0]+event_j[0]\n",
    "                else:\n",
    "                    two_seq = event_j[0]+event_i[0]\n",
    "\n",
    "                counts[two_seq] += 1\n",
    "\n",
    "    # this is followed by temporal joins between atoms in pairs, so\n",
    "    # include only unique combinations\n",
    "    # return {tuple(sorted(two_seq)) for two_seq,count in counts.iteritems() if count >= support_threshold}\n",
    "    return {tuple(sorted(two_seq)) for two_seq,count in counts.items() if count >= support_threshold}\n",
    "    \n",
    "    \n",
    "def temporal_join(element_i,element_j):\n",
    "    '''Given two elements, return a dictionary of new elements indexed by\n",
    "    their corresponding item sequences.\n",
    "    '''\n",
    "\n",
    "    join_results = _KeyDefaultDict(Element)\n",
    "    \n",
    "    for event_index_i,event_i in enumerate(element_i.events):\n",
    "        for event_index_j,event_j in enumerate(element_j.events):\n",
    "    \n",
    "            if event_i.sid == event_j.sid:\n",
    "                                        \n",
    "                sid = event_i.sid\n",
    "                superseqs = tuple()\n",
    "                superseqs_events = tuple()\n",
    "            \n",
    "                # these two atoms occur in the same sequence\n",
    "                # if they occur at different times (different eids), then\n",
    "                # their combination atom has the later eid by Corollary 1 (Zaki 2001)\n",
    "                if event_i.eid > event_j.eid:\n",
    "                    superseq = element_j.seq + tuple(element_i.seq[-1])\n",
    "                    superseq_event = Event(sid=sid,eid=event_i.eid)\n",
    "                    join_results[superseq] |= Element(superseq,superseq_event)\n",
    "\n",
    "                elif event_i.eid < event_j.eid:\n",
    "                    superseq = element_i.seq + tuple(element_j.seq[-1])\n",
    "                    superseq_event = Event(sid=sid,eid=event_j.eid)\n",
    "                    join_results[superseq] |= Element(superseq,superseq_event)\n",
    "\n",
    "                elif element_i.seq[-1] != element_j.seq[-1]:\n",
    "\n",
    "                    superseq_event = Event(sid=sid,eid=event_j.eid)\n",
    "\n",
    "                    # for coincident atoms, join the last element of one atom to the other\n",
    "                    # ensure that the itemset is sorted\n",
    "                    superseq_i = element_i.seq[:-1] + tuple([\n",
    "                        ''.join(sorted(set(element_i.seq[-1] + element_j.seq[-1])))\n",
    "                        ])\n",
    "                    join_results[superseq_i] |= Element(superseq_i,superseq_event)\n",
    "\n",
    "                    superseq_j = element_j.seq[:-1] + tuple([\n",
    "                        ''.join(sorted(set(element_i.seq[-1] + element_j.seq[-1])))\n",
    "                        ])\n",
    "\n",
    "                    # if both resulting atoms are identical, only add it once\n",
    "                    if superseq_j != superseq_i:\n",
    "                        join_results[superseq_j] |= Element(superseq_j,superseq_event)\n",
    "                \n",
    "    return join_results\n",
    "\n",
    "\n",
    "def enumerate_frequent_seq(elements,support_threshold):\n",
    "    '''Recursively traverse the sequence lattice, generating frequent n+1-length\n",
    "    sequences from n-length sequences provided in the id_list parameter.'''\n",
    "\n",
    "    frequent_elements = _KeyDefaultDict(Element)\n",
    "\n",
    "    \n",
    "    for element_index_i,seq_i in enumerate(elements.keys()):\n",
    "\n",
    "        frequent_elements_inner = _KeyDefaultDict(Element)\n",
    "            \n",
    "        # for element_index_j,seq_j in enumerate(elements.keys()[element_index_i+1:]):\n",
    "        for element_index_j,seq_j in enumerate(list(elements.keys())[element_index_i+1:]):\n",
    "            R = temporal_join(elements[seq_i],elements[seq_j])\n",
    "\n",
    "            for seq,element in R.items():\n",
    "                support = len(set([event.sid for event in element.events]))\n",
    "                if support >= support_threshold:\n",
    "                    frequent_elements_inner[seq] |= element\n",
    "\n",
    "\n",
    "        for seq,element in frequent_elements_inner.items():\n",
    "            frequent_elements[seq] |= element\n",
    "\n",
    "        for seq,element in enumerate_frequent_seq(frequent_elements_inner,support_threshold).items():\n",
    "            frequent_elements[seq] |= element\n",
    "\n",
    "    return frequent_elements\n",
    "\n",
    "\n",
    "def mine(sequences,support_threshold):\n",
    "    '''SPADE (Zaki 2001) is performed in three distinct steps:\n",
    "    1. Identify frequent single elements.\n",
    "    2. Identify frequent two-element sequences.\n",
    "    3. Identify all remaining sequences of three elements or more.\n",
    "    '''\n",
    "\n",
    "    # parse input sequences into individual item Elements\n",
    "    elements = _KeyDefaultDict(Element) \n",
    "\n",
    "    for sid,eid,itemset in sequences:\n",
    "        for item in itemset:\n",
    "            elements[tuple(item)] |= Element(tuple(item),Event(sid=sid,eid=eid))\n",
    "\n",
    "    # identify frequent single elements\n",
    "    elements = subset_to_support(elements,support_threshold)\n",
    "\n",
    "    # identify frequent two-element sequences using a horizontal database\n",
    "    freq_elements_len_eq_2 = count_frequent_two_seq(elements,support_threshold)\n",
    "\n",
    "    # generate ID lists for frequent two-element sequences discovered above\n",
    "    elements_len_eq_2 = _KeyDefaultDict(Element)\n",
    "\n",
    "    for two_seq in freq_elements_len_eq_2:\n",
    "\n",
    "        R = temporal_join(elements[tuple(two_seq[0])],elements[tuple(two_seq[1])])\n",
    "\n",
    "        for seq,element in R.items():\n",
    "            support = len(set([event.sid for event in element.events]))\n",
    "            if support >= support_threshold:\n",
    "                elements_len_eq_2[seq] |= element\n",
    "\n",
    "    # identify and generate ID lists for all remaining sequences\n",
    "    freq = enumerate_frequent_seq(elements_len_eq_2,support_threshold)\n",
    "\n",
    "    # collect all identified sequences of any length\n",
    "    for seq,element in elements_len_eq_2.items():\n",
    "        freq[seq] |= element\n",
    "\n",
    "    for seq,element in elements.items():\n",
    "        freq[seq] |= element\n",
    "\n",
    "    # return frequent sequences\n",
    "    return freq\n",
    "\n",
    "\n",
    "def read_sequences(filename):\n",
    "    '''Read sequences from a CSV.\n",
    "\n",
    "    The CSV contains one line per sequence with columns defined as follows:\n",
    "    - First column is a unique integer as sequence ID (sid)\n",
    "    - Second column is a sequence-unique integer as event ID (eid)\n",
    "    - Each remaining column contains an item as a character string with columns\n",
    "      arranged in sequence order\n",
    "    '''\n",
    "\n",
    "    import csv\n",
    "\n",
    "    sequences = []\n",
    "\n",
    "    with open(filename) as f:\n",
    "        seqreader = csv.reader(f,delimiter=',')\n",
    "        for seqline in seqreader:\n",
    "            sequences.append(\n",
    "                    tuple([ seqline[0],seqline[1],tuple(seqline[2:]) ])\n",
    "                    )\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat transactions input and output\n",
    "1. read text file into list of lists\n",
    "2. generate SID, EID, and Item (as character string)\n",
    "    * Final sequences should be a list of tuples (SID, EID, and Item)\n",
    "3. convert output into sequence; support - Pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Step 1. read \"reviews_samples.txt\" into list of lists\n",
    "# https://stackoverflow.com/questions/18448847/import-txt-file-and-having-each-line-as-a-list\n",
    "transactions = []\n",
    "with open('reviews_sample.txt', 'rt') as f:\n",
    "    for line in f:\n",
    "        transactions.append(line.strip().split(' '))\n",
    "print(type(transactions))\n",
    "print(type(transactions[0]))\n",
    "print(len(transactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "transactions = [\n",
    "    ['A','B','A','C'],\n",
    "    ['A','C','A','B','A','B'],\n",
    "    ['B','A','A','C','D']]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2. generate SID, EID, and Item (as character string)\n",
    "# Each tuple is (SID, EID, Item)\n",
    "# Add tuple to list (sequences)\n",
    "sequences = []\n",
    "for line_num, line in enumerate(transactions): # SID corresponds to line_num + 1\n",
    "    for item_num, item in enumerate(line): # EID corresponds to item_num + 1\n",
    "        tup = (line_num+1, item_num+1, item)\n",
    "        sequences.append(tup)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 'A')\n",
      "(1, 2, 'B')\n",
      "(1, 3, 'A')\n",
      "(1, 4, 'C')\n",
      "(2, 1, 'A')\n",
      "(2, 2, 'C')\n",
      "(2, 3, 'A')\n",
      "(2, 4, 'B')\n",
      "(2, 5, 'A')\n",
      "(2, 6, 'B')\n",
      "(3, 1, 'B')\n",
      "(3, 2, 'A')\n",
      "(3, 3, 'A')\n",
      "(3, 4, 'C')\n",
      "(3, 5, 'D')\n"
     ]
    }
   ],
   "source": [
    "for entry in sequences:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__._KeyDefaultDict'>\n",
      "2\n",
      "('A', 'A', 'C')\n",
      "2\n",
      "('B', 'A', 'C')\n",
      "2\n",
      "('A', 'B', 'A')\n",
      "3\n",
      "('A', 'C')\n",
      "4\n",
      "('A', 'A')\n",
      "2\n",
      "('B', 'C')\n",
      "4\n",
      "('B', 'A')\n",
      "3\n",
      "('A', 'B')\n",
      "7\n",
      "('A',)\n",
      "4\n",
      "('B',)\n",
      "3\n",
      "('C',)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Test mine()\n",
    "import pprint as pp\n",
    "\n",
    "frequent_sequences = mine(sequences, 2) # use absolute support\n",
    "print(type(frequent_sequences))\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for k, v in frequent_sequences.items():\n",
    "    print(len(v.events)) # support is num of events\n",
    "    print(k) # key is sequence represented as tuple\n",
    "    cnt+=1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, ('A', 'A', 'C')],\n",
       " [2, ('B', 'A', 'C')],\n",
       " [2, ('A', 'B', 'A')],\n",
       " [3, ('A', 'C')],\n",
       " [4, ('A', 'A')],\n",
       " [2, ('B', 'C')],\n",
       " [4, ('B', 'A')],\n",
       " [3, ('A', 'B')],\n",
       " [7, ('A',)],\n",
       " [4, ('B',)],\n",
       " [3, ('C',)]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3a. Add to list of lists\n",
    "all_frequent_sequences = []\n",
    "\n",
    "for k, v in frequent_sequences.items():\n",
    "    all_frequent_sequences.append([len(v.events), k])\n",
    "    #print(len(v.events)) # support is num of events\n",
    "    #print(k) # key is sequence represented as tuple\n",
    "    cnt+=1\n",
    "print(cnt)\n",
    "all_frequent_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>(A, A, C)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>(B, A, C)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>(A, B, A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>(A, C)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>(A, A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>(B, C)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>(B, A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>(A, B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>(A,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>(B,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>(C,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    support   sequence\n",
       "0         2  (A, A, C)\n",
       "1         2  (B, A, C)\n",
       "2         2  (A, B, A)\n",
       "3         3     (A, C)\n",
       "4         4     (A, A)\n",
       "5         2     (B, C)\n",
       "6         4     (B, A)\n",
       "7         3     (A, B)\n",
       "8         7       (A,)\n",
       "9         4       (B,)\n",
       "10        3       (C,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3b. Convert to pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# support is int, and sequence is tuple\n",
    "all_frequent_sequences_df = pd.DataFrame(all_frequent_sequences, columns=['support', 'sequence'])\n",
    "all_frequent_sequences_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Mining (Generate n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Results + Write to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3:A\n",
      "\n",
      "3:B\n",
      "\n",
      "3:C\n",
      "\n",
      "2:A;B\n",
      "\n",
      "3:A;C\n",
      "\n",
      "3:B;A\n",
      "\n",
      "2:A;B;A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq_pattern = []\n",
    "# print Formatting\n",
    "for seq_dict in result:\n",
    "    for k, v in seq_dict.items():\n",
    "        seq_pattern = [x for x in k]\n",
    "        print(str(v) + ':' + ';'.join(map(str, seq_pattern)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_pattern = []\n",
    "with open('output/patterns_test.txt', 'wt') as f:\n",
    "    for seq_dict in result:\n",
    "        for k, v in seq_dict.items():\n",
    "            seq_pattern = [x for x in k]\n",
    "            f.write(str(v) + ':' + ';'.join(map(str, seq_pattern)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "Please set the relative minimum support to 0.01 and run it on the given text file. In other words, you need to extract all the frequent contiguous sequential patterns that have an absolute support no smaller than 100.\n",
    "\n",
    "Please write all the frequent contiguous sequential patterns along with their absolute supports into a text file named \"patterns.txt\". Every line corresponds to exactly one pattern you found and should be in the following format:\n",
    "\n",
    "support:item_1;item_2;item_3\n",
    "\n",
    "For example, suppose the phrase \"parking lot\" has an absolute support 133, then the line corresponding to this frequent contiguous sequential pattern in \"patterns.txt\" should be:\n",
    "\n",
    "133:parking;lot\n",
    "\n",
    "Notice that the order does matter in sequential pattern mining. That is to say,\n",
    "\n",
    "133:lot;parking\n",
    "\n",
    "may be graded as incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run Spade to Generate Frequent, Sequential Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Step 1. read \"reviews_samples.txt\" into list of lists\n",
    "# https://stackoverflow.com/questions/18448847/import-txt-file-and-having-each-line-as-a-list\n",
    "transactions = []\n",
    "with open('reviews_sample.txt', 'rt') as f:\n",
    "    for line in f:\n",
    "        transactions.append(line.strip().split(' '))\n",
    "print(type(transactions))\n",
    "print(type(transactions[0]))\n",
    "print(len(transactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transasctions = transasctions[0:10] # test on 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2. generate SID, EID, and Item (as character string)\n",
    "# Each tuple is (SID, EID, Item)\n",
    "# Add tuple to list (sequences)\n",
    "sequences = []\n",
    "for line_num, line in enumerate(transactions): # SID corresponds to line_num + 1\n",
    "    for item_num, item in enumerate(line): # EID corresponds to item_num + 1\n",
    "        tup = (line_num+1, item_num+1, item)\n",
    "        sequences.append(tup)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform SPADE mine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-5c7876800c36>\u001b[0m in \u001b[0;36mmine\u001b[0;34m(sequences, support_threshold)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# identify frequent two-element sequences using a horizontal database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mfreq_elements_len_eq_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_frequent_two_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msupport_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;31m# generate ID lists for frequent two-element sequences discovered above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-5c7876800c36>\u001b[0m in \u001b[0;36mcount_frequent_two_seq\u001b[0;34m(elements, support_threshold)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mevent_index_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevent_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mevent_index_j\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevent_j\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_index_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mevent_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mevent_j\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "frequent_sequences = mine(sequences, 100) # use absolute support no smaller than 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3a. Add to list of lists\n",
    "all_frequent_sequences = []\n",
    "\n",
    "for k, v in frequent_sequences.items():\n",
    "    all_frequent_sequences.append([len(v.events), k])\n",
    "    #print(len(v.events)) # support is num of events\n",
    "    #print(k) # key is sequence represented as tuple\n",
    "    cnt+=1\n",
    "print(cnt)\n",
    "all_frequent_sequences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3b. Convert to pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# support is int, and sequence is tuple\n",
    "all_frequent_sequences_df = pd.DataFrame(all_frequent_sequences, columns=['support', 'sequence'])\n",
    "all_frequent_sequences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_frequent_sequences_df.to_csv(\"output/SPADE_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs498]",
   "language": "python",
   "name": "conda-env-cs498-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
